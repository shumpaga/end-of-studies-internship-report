\chapter{Scan Finder}
\label{ch:scanfinder}
This chapter describes \emph{Scan Finder}, an algorithm that can be used to retrieve all scanners position of a point cloud,  if there are any, regardless of its nature. Firstly, Section~\ref{sc:spec} reviews the context which brings this need and describes some characteristics of the expected solution. Then, Section~\ref{sc:work} reminds that there is no previous work related to this subject. Finally, Section~\ref{sc:highdens} describes a first step toward scanner location
finding before showing and discussing the results of two different approaches described respectively in Section~\ref{sc:grid-pattern} and Section~\ref{sc:elliptic}. To be clear, Section~\ref{sc:highdens} is a common first step to both methods.



\section{Specifications}
\label{sc:spec}
\subsection{Context}
\CC started to support point cloud reconstruction two (2) years ago. Today it even provides a hybrid processing mode which gives the opportuniy to combine the best of both words, photos and point clouds, in order to have a better precision. However, a recurring problem observed among \CC users is the impossibility to use the software after losing some metadata, specifically, scanners location. This is particulary problematic because usually, \CC users subcontract point cloud production to private companies which can charge them again for new exports (provided that they still have the point clouds). To enable customers to use their \emph{defective} point clouds, the graphic interface of \CC allows to specify a scanner location by hand by positioning it in a 3D representation of the point cloud. But, it does not work with merged scans; only one scanner location can be specified. Usually, users do not know the location and even if they know, the reconstruction is subjected to too much errors.

This is how the need for an algorithm to automatically find scanners positions in a point cloud is born. In addition, with such algorithm, \CC will have the possibility to enhance the set of supported file formats. Currently, it supports file formats such as \emph{PTX}, \emph{e57}, \emph{PLY}, \emph{POD}, each of them being able to store scanners positions. But not all file formats are able to do it. For instance, \emph{LAS} file format is not currently accepted as input because it does not provide any means of storing scanners location in metadata. Supporting more input file formats is also a good point of interest for \CC users.

\subsection{Objective}
In summary, the purpose here is to improve \CC point cloud support in two ways: support more file formats and allow users who lose scanners location to still use their point clouds. To do so, the algorithm must:
\begin{itemize}
  \item take as input any 3D point cloud captured from static scanners,
  \item be invariant to the number of scanners in the point cloud,
  \item be invariant to differences between scanners, such as: density, noise, rotation angle,
  \item find all scanners locations,
  \item have a reasonnable running time.
\end{itemize}

Let us emphasize here that as a first step, the algorithm is expected to work only with static point clouds. It would be difficult to have the same approach with static and mobile point clouds. Extending it to mobile point clouds is certainly the next step.



\section{Related Work}
\label{sc:work}
As said in the introduction, to the best of our knowledge, there have been no previous work on the subject.

\emph{``Detecting the positions of multiple scanners exclusively from a point cloud is a subject not identified as interesting by academics, and thus not tackled since this information is almost always available from the outset. The same holds true on the industry side, it is only recently that scanner position information is relevant for a few applications like \CC."}

The closest publications on the topic are \cite{ml1, ml2, ml3, ml4, ml5}. They use learning techniques to estimate the point of view of one particular photo based on other photos. But this belongs more to the photogrametry domain and therefore is not applicable in a point cloud context.



\section{Clustering high-density area}
\label{sc:highdens}
This section explains a common first step to methods described in Section~\ref{sc:grid-pattern} and Section~\ref{sc:elliptic}. For a better understanding, a quick introduction to LiDAR scanners is necessary.

\begin{figure}
  \centering
  \includegraphics[scale=1]{img/lidar.jpg}
  \caption{Example of a LiDAR scanner.}
  \label{fig:lidar}
\end{figure}


\subsection{LiDAR scanner and context}
\label{subsc:lidar}
LiDAR is an acronym of Light Detection and Ranging. It is a remote sensing technology which uses the pulse from a laser to collect measurements. The principle is simple: it works in a similar way to Radar and Sonar but uses light waves from a laser, instead of radio or sound waves. A LiDAR system calculates how long it takes for the light to hit an object or surface and reflect back to the scanner and then, uses the velocity of light\footnote{The velocity, or speed of light is 299,792,458 metres
per second.} to calculate the distance. LiDAR systems can fire around 1,000,000 pulses per second. This paragraph is inspired by \cite{lidar}.

When scanning, there are two kind of rotations that a LiDAR scanner performs. They are indicated by black arrows in Figure~\ref{fig:lidar}. Not only a LiDAR scanner has a constant rotation angle when turning on itself but it also has a constant vertical rotation angle when scanning the environment. Because of these constant rotation angles, the further we go, the lower the point density is. On the contrary, the closer to the scanner we are, the higher the point density is. This density variation can be observed in
Figure~\ref{fig:density}. To conclude, extracting the most dense areas is interesting as they always are around scanner locations and then, reduce the global search area.

\begin{figure}
  \centering
  \includegraphics[scale=1]{img/lidar.jpg}
  \caption{Variation of density as we go further away from the source.}
  \label{fig:density}
\end{figure}


\subsection{Clustering and dense area extraction}
\label{subsc:highdens}
\begin{figure}
  \centering
  \includegraphics[scale=1]{img/lidar.jpg}
  \caption{Result of high density extraction on several point clouds.}
  \label{fig:highdens}
\end{figure}

The first step of the algorithm is to detect high density regions in the point cloud. For each point, we compute its density based on the mean distance to its $k$ neighbors ($k \in [15,50]$). We extract high density regions by selecting a percentage of the densest points (percentage between 1 and 5\%). Figure~\ref{fig:highdens} shows some results of this high density point extraction. As you can see, they always contain the scanner. Note that the scanner is not able to scan below himself leading to an elliptic form\footnote{This elliptic form is the key point of the method described in Section~\ref{sc:elliptic}.} appearing in all point clouds.

Once these points are extracted, a clustering step is applied. The clustering is needed so that points from the point clouds corresponding to potentially different parts of the scene around the scanner can be separated. For each point, we compute its non-oriented normal and the associated \emph{confidence level} by performing a principal component analysis (PCA) on its local neighborhood. Section~\ref{subsc:pca} gives more details on how this PCA works. Then, as long as points are not attributed
to a cluster, we select the point on the flattest surface, and iteratively add its neighbors that share a coherent normal orienta-tion (the dot product of the two normals must be superior to 0.8 in our experiments). Note that the  flattest surface is determined using condifence levels, they tell which point belongs to a planar region.

Once every point is clustered, we discard clusters with too few points (in our experiments, we use a threshold of 0.2\% of the number of high density points). We then detect clusters that are circular. A cluster is considered circular if its centroid, i.e. the average of all the points it contains, is far away from the points of the cluster.

At this point, we have computed a coarse location of the scanner. Methods described in Section~\ref{sc:grid-pattern} and Section~\ref{sc:elliptic} use it in their own way in order to find scanner locations.



\section{The grid-pattern method}
\label{sc:grid-pattern}
This section explains one approach we tried in order to solve the problem. Before explaining the method in detail, let us first give the intuition behind this idea.

\subsection{Overview}
As said in Section~\ref{subsc:lidar}, LiDAR scanners perfom two kind of rotations. These constant vertical and horizontal rotation angles reveal some grid patterns on surfaces perpendicular to the ground\footnote{To be precise, perpendicular to the surface on which the scanner is installed.}. Figure~\ref{fig:grid1} shows an example of a grid pattern that can be found in point clouds. The purpose here is to find, for a single grid pattern, a relation between all constant rotation angles and the scanner's position. Therefore, this relation can be applied with all possible grid patterns in the point clouds, leading to a problem with a huge set of
constraints to solve. The idea is that only the real scanner's location is able to explain in the best way these constant rotation angles.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{img/grid1.png}
  \caption{Example of a grid pattern. As you can see there is a constant offset between each vertical line and each horizontal lines. Note that the offset between columns is not necessary the same than between lines.}
  \label{fig:grid1}
\end{figure}

This algorithm can be divided into two parts. The first step is to find \emph{accurate} grid patterns in point clouds. Once this is done, the second step is to build the equation that will be solved. These two parts are explained in the following subsections. Note that this approach does not work in multiscan mode, compare to the other approach described in Section~\ref{sc:elliptic}. It assumes there is only one scanner which explains all grid-patterns in the point cloud.

\subsection{Grid-pattern matching}
This subsection explains how to find \emph{accurate} grid patterns in point clouds in order to reduce as much as possible the noise and its impact during the problem resolution described in Section~\ref{subsc:eq}. A set of point is considered as an \emph{accurate} grid-pattern when:
\begin{itemize}
  \item it is planar as much as possible\footnote{Because of the noise.},
  \item the underlying planar surface is orthogonal to the surface of the ground,
  \item the gap between columns is consistent enough,
  \item the gap between lines is consistent enough.
\end{itemize}


\begin{algorithm}[tb]
  \SetAlgoVlined
  \DontPrintSemicolon
  \SetKw{Report}{report}
  \SetArgSty{}
  \SetKwFunction{GetPlanarPatches}{GetPlanarPatches}
  \SetKwProg{Fn}{Function}{}{}
  \Fn{\GetPlanarPatches{P}}{
    \KwIn{a pointcloud $P = \left\lbrace p_i \mid i \in [0, s]  \right\rbrace$.}
    \KwOut{a set of potential grid-pattern patches.}
    \tcc{We compute the normal of each point and keep both confidence level and normal $\left\langle \mu_i, \vec{v}_i \right\rangle$.}
    \tcc{See Section~\ref{subsc:pca} for more details on PCA algorithm.}
    \tcc{From here, $\forall i \in [0, s]$ we assume $\vec{v}_i$ to be the normal at $i$ and $\mu_i$ its confidence level.}
    \For{$i = 0;\ i < s;\ i = i + 1$}{
      $\left\langle \mu_i, \vec{v}_i \right\rangle \gets PCA($50 closest neighbours of $i)$\;
    }
    $R\gets\emptyset$\;
    visited $\gets\emptyset$\;
    \For{$i = 0;\ i < s;\ i = i + 1$}{
      \tcc{ Continue if already visited or is not planar enough.}
      \If{$i \in$ visited or $\mu_i > 0.001$}
      {\Continue}
      tmp $\gets \left\lbrace p_i \right\rbrace$\;
      \ForEach{$j \in $ 50 closest neighbours of $i$}{
        \tcc{If $\vec{v}_j$ and $\vec{v}_i$ are almost colinear and $j$ has not been seen yet.}
        \If{abs($\vec{v}_j \times \vec{v}_i$) $> 0.9$ and $j \not\in$ visited}{
          tmp $\gets$ tmp $\cup\ p_j$\;
          visited $\gets$ visited $\cup\ j$\;
        }
      }
      \tcc{If there is enough point for a grid-pattern patch.}
      \If{size of tmp $> 0.9\ \times$ 200}{
        $R \gets R\ \cup $ tmp
      }
    }
    \Return{$R$}\;
  }
  \caption{Find various not-overlaping planar patches in a point cloud.
    \label{alg:GetPlanarPatches}}
\end{algorithm}

Algorithm~\ref{alg:GetPlanarPatches} shows how to extract some planar patches in a point cloud. A preprocessing step already done and explained in Section~\ref{subsc:highdens} is to compute for each point of the point cloud, the normal and its \emph{confidence level}. This \emph{confidence level} plays a key role in the algorithm as it tells how planar the region around each point is. By browsing all points, each time we find a point with a good \emph{confidence level} ($> 0.001$), we start to build a patch around it. To do so, we extract its 50
closest neighbours and consider only those having their normal almost colinear to the normal of the starting point. If the size of the obtained set is bigger enough ($>\ 0.9 \times 200$), the set is kept and otherwise, not. Also, in order to avoid overlaping or patches too close, we keep track of visited points.

Although planar, this set of patches potentially contains: \textbf{(a)} patches whose underlying surfaces are not orthogonal to the surface of the ground and \textbf{(b)} patches without grid patterns. To filter out each \textbf{(a)} patch, we use a dot product between its normal and the normal of the circular cluster found in the high-density area\footnote{The circular cluster desribes the surface on which the scanner is positioned.} (see Section~\ref{sc:highdens}). For \textbf{(b)} patches,
the objectif is to indentify grid-patterns. To do so, we use least square method in order to fit lines. Least-square is explained in Section~\ref{subsc:least}. Fitting a line fall into linear least square problems. Figure~\ref{fig:fitline} shows how to fit a line to a set of points using the Eigen library~\cite{eigenweb}. The purpose is to use this line fitting in order to cluster lines and columns. Figure~\ref{fig:line-col-cluster} shows results of a clustering performed on two patches after several fittings and adjustements. We only keep firstly patches containing a grid-pattern and then, patches having a regular gap between columns and lines. The rest is discarded.

\begin{figure}
  \centering
  \begin{lstlisting}
    Eigen::VectorXf FitLine(std::vector<A3D::Point_3dPlus> const& points)
    {
      Eigen::MatrixXf mat(points.size(), 2);
      Eigen::VectorXf vec(points.size());
      int count = 0;
      for (auto it = points.begin(); it != points.end(); ++it)
      {
        mat(count, 0) = static_cast<float>(it->x());
        mat(count, 1) = static_cast<float>(it->y());
        vec(count++) = 1.0;
      }
      return mat.fullPivHouseholderQr().solve(vec);
    }
  \end{lstlisting}
  \caption{Function using least-square to fit a line to the given set of points.}
  \label{fig:fitline}
\end{figure}

\begin{figure*}[t!]
  \centering
    \begin{subfigure}[t]{0.5\textwidth}
      \centering
        \includegraphics[height=2.5in]{img/grid-lines.png}
        \caption{Lines clustering performed on two patch.}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.5\textwidth}
      \centering
        \includegraphics[height=2.5in]{img/grid-cols.png}
        \caption{Column clustering performed on two patches.}
    \end{subfigure}
    \caption{Example of columns and lines clustering in order to identify grid-patterns.}
    \label{fig:line-col-cluster}
\end{figure*}

Once only \emph{accurate} patches are kept, columns and lines are identified, the problem can be formalised.

\subsection{Equation to solve}
\label{subsc:eq}

For a little recall of Section~\ref{sc:grid-pattern}, the purpose here is to find a relation between the scanner's position (what we are looking for) and the regular gaps between lines and columns. Again, the gap between lines and the gap between columns is not necessary the same. Look at Figure~\ref{fig:topview}. There are four (4) vertical lines viewed from a bird's eye: $a$, $b$, $c$ and $d$. They are represented as points because of the point of view. The regular gap between
those lines is represented by the relation $d_1 = d_2$.

FIXME: More explanation.

So, to conclude, for each grid-pattern, and for each pair of two consecutive lines (or columns), we minimize the absolute difference between their respective angles (here $\alpha_1$ and $\alpha_2$). This is a non-linear least-square problem.

\begin{figure}
  \centering
  \begin{tikzpicture}
    \draw[fill] (0,0) circle (2pt) coordinate (a) node[above=0.2cm]{$a$};
    \draw[fill] (2.5,0) circle (2pt) coordinate (b) node[above=0.2cm]{$b$};
    \draw[fill] (5,0) circle (2pt) coordinate (c) node[above=0.2cm]{$c$};
    \draw[fill] (7.5,0) circle (2pt) coordinate (d) node[above=0.2cm]{$d$};
    \draw[fill] (3.75, -4.5) circle (2pt) coordinate (e) node[below=0.1cm,black]{$s$};
    \draw[red, <->, shorten <= 0.2cm, shorten >= 0.2cm]
      (a) -- node[above]{$d_1$} (b);
    \draw[red, <->, shorten <= 0.2cm, shorten >= 0.2cm]
      (c) -- node[above]{$d_2$} (d);
    \draw[dashed] (a) -- (e) -- (b)
      pic["$\alpha_1$", draw=red, red, solid, <-, angle eccentricity=1.2,
          angle radius=1cm] {angle=b--e--a};
    \draw[dashed] (c) -- (e) -- (d)
      pic["$\alpha_2$", draw=red, red, solid, <-, angle eccentricity=1.2,
          angle radius=1cm] {angle=d--e--c};
  \end{tikzpicture}
  \caption{A top view over four (4) vertical grid lines ($a$, $b$, $c$ and $d$) and a scanner ($s$).}
  \label{fig:topview}
\end{figure}

\subsection{Results and discussions}
Not multiscan.


\section{The elliptic method}
\label{sc:elliptic}

\subsection{Overview}


\subsection{Fitting ellipse}


\subsection{Equation to solve}


\subsection{Results and discussions}
