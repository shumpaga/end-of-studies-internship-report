\chapter{Background}
\label{ch:background}
This chapter introduces the necessary concepts and definitions for a better understanding of the report.

\section{Point Cloud}
\label{sc:back-point-cloud}
\begin{definition}{: Point}
  \\A point $p_i$ is a tuple $\left\langle x_i, y_i, z_i, \vec{n}_i \right\rangle$ where:
  \begin{itemize}
    \item $i$ is an unique integer identifying $p_i$,
    \item $x_i$, $y_i$ and $z_i$ are the coordinates of $p_i$,
    \item $\vec{n}_i$ is the normal at $p_i$.
  \end{itemize}
\end{definition}

\begin{definition}{: Point Cloud}
  \\A point cloud $P$ of size $s$ is a set $P = \left\lbrace p_i \mid i \in [0, s]  \right\rbrace$.
\end{definition}

A point cloud is said to be whether \emph{static} or \emph{mobile} depending on the \emph{static} nature of the scanner. Mobile scanner means that the instrument is either handheld (like some units used for LIDAR speed detection) or mounted on a vehicle (from SUVs to trailer-mounted instruments to airborne instruments). It generates a point cloud by following a trajectory. Static scanner instead means that the scanner is set up in a location and turns on itself.


\section{Core algorithms}

\subsection{Principal Component Analysis (PCA)}
\label{subsc:pca}
PCA is a common algorithm of data analysis. It is used to project data with $n$ dimensions into a space with reduced dimensions while keeping the most relevant information -- the directions where there is the most variance, where the data is most spread out. As its name suggests, it finds the principal components from the most important to the less one. We say: the first principal component is a rotation of $x$-axis to maximize the variance of the data projected onto it, the last axis
(component) is the one with less variance thus with redundant information.

We are not going to explain in detail what a PCA is as it is very common and not the core of our work here. But let us remind the methodology in the specific case of 3D data.

\begin{definition}{: Principal Component Analysis (PCA)}
\\ Let $P_a$ be a set of points of size $s_a$. To perform \textit{PCA($P_a$):}
\begin{itemize}
\item we compute the centroid of $P_a$, noted $\bar{p_a}$
\item we compute the covariance matrix $C$ which is the symmetric $3 \times 3$ positive semi-definite matrix:
\begin{equation*}
C = \sum_{p \in P_a} (p - \bar{p_a}) \otimes (p - \bar{p_a}) \text{,       where $\otimes$ denotes the outer product vector operator.}
\end{equation*}
\item we compute the eigenvalues of $C$ with their corresponding eigenvectors. The eigenvector $\vec{v}_1$  associated with the greatest eigenvalue $\lambda_1$  is the principal component axis. And the axis having less variation of data, when it is projected onto it is $\vec{v}_3$ ; the one associated with the lowest eigenvalue $\lambda_3$.
\item we return a pair of $\left\langle \mu, \vec{v}_3 \right\rangle$ where $\mu = \frac{\lambda_3}{\lambda_1 + \lambda_2 + \lambda_3}$ can be used as a confidence level. The smaller it is, the more $\vec{v}_1$ and $\vec{v}_2$  explain on their own our data.
\end{itemize}
\end{definition}

In \cite{hoppe}, Hoppe et al. use PCA to determine the normal of a tangent plane. Similarly in this report, we use PCA for two purposes: detect if a set of points are planar by computing their normal's direction or simply find the normal's direction at a specific point using its neighbors. If a set of points forms a plane, we choose its normal to be either $\vec{v}_3$ or $-\vec{v}_3$. It is the direction with less data variation and in a perfect word -- a perfect plane alignment, the projection of all points onto it gives the same value. This means that  $\mu = 0$. Now if $P_a$ is formed of $p_i$ and its $k$-neighboring points, then $\vec{v}_3$ or $-\vec{v}_3$ is the normal at $p_i$. We empirically deduced fifty (50) to be the ideal value for $k$.


\subsection{Least Square Methods}
\label{subsc:least}
Least Square solving is a usual approach of regression analysis to approximate the solution of over-determined systems (with more equations than unknowns). The problem can be described as follows. Let us assume a model of the form
\begin{equation}
  y = f(z; x_1; ..., x_n)
  \label{eq:leastmodel}
\end{equation}
where $f$ describes a relation between $x_1, ..., x_n$, $z$ is the control variable and $y$ is the expected response to $z$. After $m$ experiments, $(m \geq n)$, $m$ observed quantities $(z_i, y_i), i \in [1, m]$ are collected. The purpose therefore is to find the parameters $x_1, ..., x_n$ so that all $z_i, y_i$ satisfy at best (\ref{eq:leastmodel}) . ``Least squares" means that the overall solution minimizes the sum of the squares of the residuals made in the results of every single equation.
The most important application is in data fitting. The best fit in the least-squares sense minimizes the sum of squared residuals, a residual being: the difference between an observed value, and the fitted value provided by a model.

Least-squares fall into two categories: linear and non-linear least squares, depending on whether or not the residuals are linear in all unknowns.

\subsubsection{Linear}
To solve linear least square problems, we use the Eigen library~\cite{eigenweb}. Consider an over-determined system of equations, say $Ax = b$. Although it has no precise solution, it makes sense to search for the vector $x$ which is closest to being a solution, in the sense that the difference $Ax - b$ is as small as possible. This $x$ is called the least square solution (if the Euclidean norm is used). Provide Eigen with $A$ and $b$ and it will approximate the solution $x$.

\subsubsection{Non-linear}
In case of non-linear least square problems, we use the Ceres Solver~\cite{ceres}. Ceres Solver is an open source C++ library for modeling and solving large, complicated optimization problems. Ceres can solve bounds constrained robustified non-linear least squares problems of the form:
\begin{equation*}
  \min_{x}\ \ \frac{1}{2} \sum_{i} \rho_i \left(\left\lVert f_i(x_{i_1}, ..., x_{i_k})  \right\rVert^2 \right) \text{, such that  } l_j \leq x_j \leq u_j
\end{equation*}

Ceres solver considers the expression $\rho_i \left(\left\lVert f_i(x_{i_1}, ..., x_{i_k})  \right\rVert^2 \right)$ as a \emph{Residual Block}, where $f_i(.)$ is a \emph{Cost function} that depends on the parameter blocks $[x_{i_1}, ..., x_{i_k}]$.
We only need to specify the parameters, write the \emph{Cost function} and Ceres does the job.


\subsection{RANSAC}
\label{subsc:ransac}
According to \cite{ransac}:
RANSAC is the abbreviation of Random Sample Consensus. It is a general algorithm that can be used with other parameter estimation methods in order to obtain robust models with a certain probability when the noise in the data doesn't obey the general noise assumption.

Various versions of this algorithm exist, this is why it is said to be \emph{general}. Algorithm~\ref{alg:RANSAC} shows the version used in this report. Generally, RANSAC selects a subset of data samples and uses it to estimate model parameters. Then it determines the samples that are within an error tolerance of the generated model. These samples are considered as agreed with the generated model and called as consensus set of the chosen data samples. Here, the data samples in the consensus as
behaved as inliers and the rest as outliers by RANSAC. If the count of the samples in the consensus is high enough, the new model is saved. Actually, it repeats this process for a number of iterations and returns the model which has the smallest average error among the generated models.

\begin{algorithm}[tb]
  \SetAlgoVlined
  \DontPrintSemicolon
  \SetKw{Report}{report}
  \SetArgSty{}
  \SetKwFunction{RANSAC}{RANSAC}
  \SetKwProg{Fn}{Function}{}{}
  \Fn{\RANSAC{$P$, $k$, $n$, $t$}}{
    \KwIn{
      \begin{itemize}
        \item a set of points $P = \left\lbrace p_i \mid i \in [0, s]  \right\rbrace$,
        \item the number $k$ of iteration,
        \item the required sample number $n$ for fitting parameters to the mathematical model,
        \item the tolerated error threshold $t$ when testing parameters on one sample.
      \end{itemize}
    }
    \KwOut{A tuple $\left\langle m, l\right\rangle$ where $m$ is the final parameter estimation and $l$ the associated set of inliers.}
    \;
    $m \gets \vec{0}$ \tcp{parameter vector}
    $l \gets \emptyset$ \tcp{inliers set}
    $\delta_n \gets \emptyset$ \tcp{set of extracted samples}
    \For{$i = 0;\ i < k;\ i = i + 1$}{
      $\delta_n \gets$ random\_sample$(P, n)$ \tcp{Extract $n$ random points from $P$}
      $m' \gets$ fit\_parameters$(n, \delta_n)$ \tcp{Use an estimation method to fit $m'$ to the mathematical model}
      $l'\gets \emptyset$\;
      \For{$j = 0;\ j < s;\ j = j + 1$}{
        $e \gets$ compute\_error$(m', p_j)$ \tcp{get model error when applied on point $p_j$}
        \If{$e^2 \leq t^2$} {
          $l' \gets l' \cup \left\lbrace p_j \right\rbrace$
        }
      }
      \tcc{If it is the most satisfied mathematical model up to now}
      \If{size\_of$(l') \leq$ size\_of$(l)$} {
        $l \gets l'$\;
        $m \gets m'$\;
      }
    }
    \Return{$\left\langle m, l \right\rangle$}\;
  }
  \caption{A variation of the RANSAC algorithm used in this report
    \label{alg:RANSAC}}
\end{algorithm}

